## {{ ansible_managed }}
# Parse & upload the elasticsearch log4J formatted logs for easier access
#  example1: [2016-10-08 14:57:07,674][INFO ][monitor.jvm              ] [sl73ovnapd024] [gc][young][1696665][262630] duration [925ms], collections [1]/[1.6s], total [925ms]/[8.4h], memory [4gb]->[3.7gb]/[15.9gb]
#  example2: [2016-10-08 15:59:45,361][WARN ][cluster.action.shard     ] [sl73ovnapd024] can't send shard failed for [kafka-2016.10.08][1], node[IVGo6pryQ8S7y8luircCrg], [R], v[3], s[STARTED], a[id=9lP15z3gSYu2Oe
#  example3: [2016-10-08 16:15:20,895][WARN ][cluster.action.shard     ] [sl73ovnapd024] [ovnswitch-syslog-2016.10.08][3] received shard failed for [ovnswitch-syslog-2016.10.08][3], node[usK5N4onSl-IgpU8uE6zPg]

[LogstreamerInput_elasticsearch]
type="LogstreamerInput"
log_directory = "/var/log/elasticsearch"
file_match = 'elasticsearch\.log'
decoder = "elasticsearch_multidecoder"
splitter = "SplitterForElasticsearchLogs"

[SplitterForElasticsearchLogs]
type = "RegexSplitter"
delimiter = '\n(\[\d{4}-\d{2}-\d{2}[ T]\d{2}:\d{2}:\d{2},\d{3}\])'
delimiter_eol = false

##### decode in two steps
[elasticsearch_multidecoder]
type = "MultiDecoder"
subs = ['elasticsearch_standard_decoder', 'elasticsearch_exception_decoder']
cascade_strategy = "first-wins"

[elasticsearch_standard_decoder]
type = "PayloadRegexDecoder"
match_regex = '\n?\[(?P<Timestamp>(?P<Date>\d{4}-\d{2}-\d{2})[ T](?P<Time>\d{2}:\d{2}:\d{2})),(?P<millisecs>\d{3})\]\[(?P<Severity>[A-Z]+)\s*\]\[(?P<Xclass>\S+)\s*] \[(?P<Xhostname>\S+)\s*\] \[(?P<Xindexname>\S+\d{4}\.\d{2}\.\d{2})\](?P<Xmessage>[^$]+)'
log_errors = true
timestamp_layout = "2006-01-02T15:04:05"
timestamp_location = "UTC"
[elasticsearch_standard_decoder.severity_map]
TRACE = 8
DEBUG = 7
INFO = 6
NOTICE = 5
WARN = 4
ERROR = 3
CRITICAL = 2
[elasticsearch_standard_decoder.message_fields]
Payload = "%Xmessage%"
esclass = "%Xclass%"
eshost = "%Xhostname%"
esindex = "%Xindexname%"
logTimestamp = "%Date%T%Time%.%millisecs%Z"
Type = 'elasticsearch.log'

[elasticsearch_exception_decoder]
type = "PayloadRegexDecoder"
match_regex = '\n?\[(?P<Timestamp>(?P<Date>\d{4}-\d{2}-\d{2})[ T](?P<Time>\d{2}:\d{2}:\d{2})),(?P<millisecs>\d{3})\]\[(?P<Severity>[A-Z]+)\s*\]\[(?P<class>\S+)\s*] (?P<Xmessage>[^$]+)'
log_errors = true
timestamp_layout = "2006-01-02T15:04:05"
timestamp_location = "UTC"
[elasticsearch_exception_decoder.severity_map]
TRACE = 8
DEBUG = 7
INFO = 6
NOTICE = 5
WARN = 4
ERROR = 3
CRITICAL = 2
[elasticsearch_exception_decoder.message_fields]
Payload = "%Xmessage%"
logTimestamp = "%Date%T%Time%.%millisecs%Z"
Type = 'elasticsearch.log'

[ESJsonEncoder_elasticsearch]
type="ESJsonEncoder"
es_index_from_timestamp = true
{% raw %}
index = "elasticsearchlog-%{%Y.%m.%d}"
{% endraw %}

[ElasticSearchOutput_elasticsearch]
type="ElasticSearchOutput"
message_matcher = '(Logger == "LogstreamerInput_elasticsearch") && (Severity <= 6)'
{% if elasticsearch_client_ssl_enabled == "true" %}
server = "https://{{ elasticsearch_host }}:{{ elasticsearch_port }}"
{% else %}
server = "http://{{ elasticsearch_host }}:{{ elasticsearch_port }}"
{% endif %}
flush_interval = 5000
flush_count = 100
encoder = "ESJsonEncoder_elasticsearch"
    {% if elasticsearch_client_ssl_enabled == "true" %}
    [ElasticSearchOutput_elasticsearch.tls]
    cert_file = "/etc/pki/tls/elasticsearch-client/certs/{{ inventory_hostname }}.pem"
    key_file = "/etc/pki/tls/elasticsearch-client/private/{{ inventory_hostname }}.pem"
    root_cafile = "/etc/pki/tls/elasticsearch-client/certs/bundle.pem"
    client_auth = "RequireAndVerifyClientCert"
    prefer_server_ciphers = true
    min_version = "TLS12"
    {% endif %}



