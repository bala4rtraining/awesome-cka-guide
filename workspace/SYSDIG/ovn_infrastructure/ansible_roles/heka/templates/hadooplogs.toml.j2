## {{ ansible_managed }}

[LogstreamerInput_hadoop]
type="LogstreamerInput"
log_directory = "/var/log/hadoop-hdfs"
file_match = 'hadoop-hdfs-(?P<logname>\S+)\.log'
differentiator = ["hadoop-hdfs-", "logname", ".log"]
decoder = "hadoop_transform_decoder"
splitter = "SplitterForHadoopLogs"

[SplitterForHadoopLogs]
type = "RegexSplitter"
delimiter = '\n(\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}\])'
delimiter_eol = false

[hadoop_transform_decoder]
type = "PayloadRegexDecoder"
match_regex = '[\n]?\[(?P<Timestamp>(?P<Date>\d{4}-\d{2}-\d{2}) (?P<Time>\d{2}:\d{2}:\d{2})),(?P<millisecs>\d{3})\] \[(?P<Severity>[A-Z]+)\] \[(?P<class>\S+)\] (?P<Xmessage>[^$]+)'
timestamp_layout = "2006-01-02 15:04:05"
timestamp_location = "UTC"
[hadoop_transform_decoder.severity_map]
TRACE = 8
DEBUG = 7
INFO = 6
NOTICE = 5
WARN = 4
ERROR = 3
CRITICAL = 2
[hadoop_transform_decoder.message_fields]
Payload = "%Xmessage%"
logTimestamp = "%Date%T%Time%.%millisecs%Z"

[ESJsonEncoder_hadoop]
type="ESJsonEncoder"
es_index_from_timestamp = true
{% raw %}
index = "hadoop-%{%Y.%m.%d}"
{% endraw %}

[ElasticSearchOutput_hadoop]
type="ElasticSearchOutput"
message_matcher = '(Logger =~ /^hadoop/) && (Severity <= 6)'
{% if elasticsearch_client_ssl_enabled == "true" %}
server = "https://{{ elasticsearch_host }}:{{ elasticsearch_port }}"
{% else %}
server = "http://{{ elasticsearch_host }}:{{ elasticsearch_port }}"
{% endif %}
flush_interval = 5000
flush_count = 100
encoder = "ESJsonEncoder_hadoop"
    {% if elasticsearch_client_ssl_enabled == "true" %}
    [ElasticSearchOutput_hadoop.tls]
    cert_file = "/etc/pki/tls/elasticsearch-client/certs/{{ inventory_hostname }}.pem"
    key_file = "/etc/pki/tls/elasticsearch-client/private/{{ inventory_hostname }}.pem"
    root_cafile = "/etc/pki/tls/elasticsearch-client/certs/bundle.pem"
    client_auth = "RequireAndVerifyClientCert"
    prefer_server_ciphers = true
    min_version = "TLS12"
    {% endif %}
